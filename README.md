# ava
a personal healthcare assistant

Ava makes use of the following technologies:

Ava’s front end was designed in:
1.	Bootstrap
2.	CSS 
3.	HTML5
4.	JavaScript

Ava’s server communications are handled by:
1.	Node.js

We use this hardware to assist Ava:
2.	Muse Headband (Brain wave sensor)
3.	Oculus

Ava’s Brain:
1.	VR Development
2.	Cloud Speech API
3.	Dialogflow Enterprise Edition 
4.	Natural language processing
5.	MATLAB
6.	EEG module (3D modelling of muse sensor data)
7.	AI
8.	Machine Learning
9.	APIs
10.	Big Data

Working:
The user visits our website/app and is taken through a simple questionnaire where he’s asked a set of questions designed by medical professionals to ascertain the user’s mental state, and information about the user’s lifestyle and health. The data entered by him will be collected and stored as part of his user profile. Every interaction the user has with the intelligent assistant “Ava” builds upon that. If the user preliminarily shows signs of distress, then they are requested to visit a healthcare provider to undergo further testing/diagnosis. If the user showed a likelihood for suspected mental health issues, certain questions that aid their diagnosis, designed by psychologists are presented to the user on the website while collecting EEG data through the Muse Headband. The EEG data points are used to generate a 3D brain activity heat map, which can be used to identify potential fear responses to certain stimuli, aiding in diagnosis. Finally, this also serves as a platform to help alleviate certain conditions through special exposure therapy designed on the VR platform, in conjunction with progress indicated via lower fear responses collected via the Muse Headband. 
